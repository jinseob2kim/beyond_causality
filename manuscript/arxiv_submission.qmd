---
title: "Epidynamix: From Force to Field in Real-World Epidemiology"
author:
  - name: Jinseob Kim
    affiliations:
      - name: Zarathu Co., Ltd.
        city: Seoul
        country: Korea
    email: jinseob2kim@gmail.com
format:
  pdf:
    documentclass: article
    geometry: margin=1in
    fontsize: 11pt
    linestretch: 1.5
    keep-tex: true
    include-in-header:
      text: |
        \usepackage{amsmath}
        \usepackage{amssymb}
        \usepackage{hyperref}
bibliography: references.bib
keywords:
  - causal inference
  - real-world data
  - positivity violation
  - treatment effect heterogeneity
  - risk potential field
abstract: |
  Standard causal inference methods estimate average treatment effects—scalars that work well when effects are homogeneous and positivity holds. In Real-World Data (RWD), however, clinical guidelines create structural positivity violations, and effects vary systematically across patient states. We propose Epidynamix, a complementary output format that maps treatment effect heterogeneity explicitly. Defining a Risk Potential Field $\Phi_a(x) = -\log\lambda(x,a)$ for each treatment regime, we compute the effect surface $\Delta_A\Phi(x) = \Phi_1(x) - \Phi_0(x)$—mathematically equivalent to CATE on the log-hazard scale. The key difference is interpretive: positivity violations become structural boundaries to report, not estimation failures to fix. Through simulation, we show that while Cox regression and IPTW yield valid averages (HR = 0.55), the effect surface reveals ~3.5-fold variation across state space, with the largest benefits in high-risk patients where guidelines mandate treatment. This reframing does not replace causal inference but enriches its output: **a map, not just a number.**
---

# Introduction: When Averages Are Not Enough

Modern causal inference in epidemiology typically asks:

$$
\text{ATE} = \mathbb{E}[Y(1) - Y(0)]
$$

This formulation assumes: well-defined potential outcomes $Y(a)$, interventions acting as external causes, and counterfactual symmetry between $a=0$ and $a=1$. This works extremely well in randomized controlled trials.

However, in Real-World Data (RWD), the system is high-dimensional, time-varying, policy- and guideline-constrained, and observer-influenced. Yet we still model intervention as a force:

$$
A \;\longrightarrow\; Y
$$

**Why now?** The explosion of RWD—electronic health records, claims databases, registries—has exposed the limitations of this paradigm. Unlike curated trial populations, RWD reflects the full complexity of clinical practice: treatment decisions are constrained by guidelines, contraindications create structural zeros in the propensity score, and effects vary systematically across patient states [@petersen2012]. We increasingly face situations where:

- Treatment is mandated for high-risk patients (structural positivity violation)
- The "same" intervention varies by context, timing, and patient state
- Treatment decisions are entangled with prognosis (confounding by indication)

Traditional methods respond by restricting analysis to "overlap regions" or trimming extreme weights—effectively discarding the very patients for whom treatment decisions matter most.

We propose an alternative: **a map, not a number.**


# The Geometric Turn: From Force to Field

Physics offers a useful analogy. In Newtonian mechanics, gravity is a force acting between masses. In general relativity, gravity emerges from the geometry of spacetime—objects follow geodesics in a curved manifold. This is not merely a notational change; it enabled new predictions (light bending, gravitational waves).

We propose a *conceptual reframing*, not a paradigm shift of equal magnitude. The analogy is motivational, not literal:

- View treatment effects as properties of a structured state space
- Represent heterogeneity as geometry rather than residual variance
- Treat structural constraints as boundaries to be mapped, not estimation failures

This reframing does not generate new predictions from the same data. Its value lies in changing *what we report*: a map of effect heterogeneity and structural boundaries, rather than a single number.


## Notation

Throughout this paper, we use the following conventions (discrete treatment):

| Symbol | Meaning |
|--------|---------|
| $S_t = X_t$ | System state (covariates) at time $t$ |
| $\mathcal{S}$ | State space for $X$ |
| $\lambda(x, a, t)$ | Instantaneous hazard at state $x$ under $a$ at time $t$ |
| $\Phi_a(x)$ | Risk potential under regime $a\in\{0,1\}$: $-\log \lambda(x,a, t^*)$ or cumulative variant |
| $\nabla_x \Phi_a$ | Gradient w.r.t. continuous covariates $x$ (no derivative in $a$) |
| $\Delta_A \Phi(x)$ | Finite difference across regimes: $\Phi_1(x) - \Phi_0(x)$ |
| $K_a(x_t, x_{t+1})$ | Transition kernel conditional on $a_t$ |
| $\mathcal{I}_a$ | Regime selection: $x \mapsto \Phi_a(x)$ |

Notes:
- We treat $a$ as a regime index, not as a coordinate of the state. For binary $a$, treatment-axis changes use finite differences $\Delta_A \Phi$ rather than derivatives.
- Time handling is made explicit via $t$ or a chosen reference $t^*$ (see Time Treatment).


# State Space and Risk Field

## State Definition

We define the observable system state as covariates only:

$$
S_t = X_t \in \mathcal{S}.
$$

Treatment $A_t$ is a regime index applied to the system, not a coordinate of $X_t$. The dynamics of covariates can depend on treatment via a treatment-specific transition kernel:

$$
P(X_{t+1} \mid X_t, A_t=a) = K_a(X_t, X_{t+1}).
$$

Regions where $K_a$ assigns zero probability are **structural** (e.g., guideline-constrained), not estimation failures.


## Risk Potential Function

Define a local hazard field $\lambda(x,a,t)$ and the corresponding potential:

$$
\Phi_a(x) = -\log\lambda(x,a,t^*)\quad\text{(landmark)}\qquad \text{or}\qquad \Phi_a(x) = -\log\int_0^\tau \lambda(x,a,t)\,dt\;\text{(cumulative)}.
$$

Interpretation: low $\Phi_a$ → high risk; high $\Phi_a$ → relative stability. $\Phi_a$ is a **geometric property of the state space under regime $a$**.


## Time Treatment

RWD hazards are typically time-varying. We use one of:

1. Landmark: $\Phi_a(x) := -\log \lambda(x,a, t^*)$ at a clinically meaningful $t^*$
2. Discrete-time: estimate $\lambda_t(x,a)$ on bins $t\in\{1,\dots,T\}$ and summarize $\Phi_{a,t}(x)$
3. Cumulative: $\Phi_a(x) := -\log \int_0^\tau \lambda(x,a,t)\,dt$

Examples use (1) for simplicity. For applied analyses, we recommend (1) or (2), reporting $t$ explicitly and avoiding a global stationarity assumption.


# Intervention as Regime Selection

Instead of viewing $\text{do}(A=a)$ as an external force, we treat intervention as selecting which potential surface to evaluate:

$$
\mathcal{I}_a : x \mapsto \Phi_a(x).
$$

This is not a state transformation but a regime selection—choosing which of the two surfaces $\{\Phi_0, \Phi_1\}$ applies to a given covariate configuration $x$.


# Vector Field Interpretation

## Gradient of the Potential

Each $\Phi_a(x)$ defines a scalar field on $\mathcal{S}$. Its spatial gradient defines a **vector field** over $x$:

$$
\nabla_x \Phi_a(x) = \left( \frac{\partial \Phi_a}{\partial x_1}, \ldots, \frac{\partial \Phi_a}{\partial x_p} \right).
$$

For binary treatment, changes across $a$ use the finite difference $\Delta_A\Phi(x) = \Phi_1(x) - \Phi_0(x)$; no derivative in $a$ is implied.

**Connection to Cox regression:** In a log-linear Cox model, $\log \lambda(x,a,t)=\log h_0(t)+\beta_x' x + \beta_A a$, so $\nabla_x \Phi_a(x) = -\beta_x$. The field framework generalizes this to nonlinear $\Phi_a(x)$.


## Treatment Contrast as Surface Difference

For a given covariate configuration $x$, the treatment effect is the difference between the two potential surfaces:

$$
\Delta_A \Phi(x) = \Phi_1(x) - \Phi_0(x).
$$

Interpretation:

- **Protective effect:** $\Delta_A \Phi(x) > 0$ (treatment lowers hazard)
- **Harmful effect:** $\Delta_A \Phi(x) < 0$ (treatment increases hazard)

This effect varies across $x$—it is a surface, not a scalar. The spatial gradient $\nabla_x \Phi_a$ describes how risk changes with covariates within a fixed regime.


# ATE as a First-Order Projection

The true objects are the potential surfaces $\Phi_0(x)$ and $\Phi_1(x)$.

**For binary treatment** $A \in \{0,1\}$, the treatment-contrast surface is:

$$
\Delta_A \Phi(x) = \Phi_1(x) - \Phi_0(x).
$$

Averaging over $X$ yields the ATE:

$$
\text{ATE} = \mathbb{E}_X[\Delta_A \Phi(X)].
$$

On the survival scale, $\Delta_A \Phi(x) = \log\{\lambda(x,0)/\lambda(x,1)\}$ is the log hazard-ratio field; the HR field is $\exp\{-\Delta_A \Phi(x)\}$.

Identification requires (i) exchangeability $Y^a \perp A \mid X$ and (ii) positivity on the target domain. Where positivity fails, $\Phi_0(x)$ or $\Phi_1(x)$ is undefined, and $\Delta_A\Phi(x)$ is not identified—these regions are reported as structural boundaries.

**Case 1: Homogeneous effects** ($\text{Var}_X[\Delta_A \Phi] \approx 0$)

When $\Delta_A \Phi(x) \approx c$ (constant across $x$), ATE $\approx c$. No information is lost.

**Case 2: Heterogeneous effects** ($\text{Var}_X[\Delta_A \Phi]$ large)

ATE remains well-defined but collapses a distribution into a single number, losing information about where effects are strong vs weak, positive vs negative.

**Case 3: Positivity violation** (structural discontinuity)

When $P(A = 0 \mid X = x) = 0$ for some $x$, the surface $\Phi_0(x)$ is undefined in that region. ATE involves integration over regions where $\Phi_0(x)$ does not exist. This is not an estimation problem—it is a **domain problem**.


# Identification and Positivity

We do not avoid counterfactual reasoning; we avoid unnecessary notation. Estimation of $\Phi_a(x)$ and $\Delta_A\Phi(x)$ still relies on exchangeability and positivity on the support of $X$. Where $P(A=a\mid X=x)=0$, $\Phi_a(x)$ is not defined—this is a domain issue. In the field view, such regions are **disconnected** parts of $\mathcal{S}$ and are mapped as structural boundaries rather than imputed.


# Relation to MSM and the g-formula

## The g-formula as Integrated Flow

The g-formula computes the expected accumulation of local risk along trajectories constrained to follow a specific path through state space.

## MSMs as Average Projections

MSMs estimate the average directional flow of the system through the risk field, projected onto the treatment axis [@cole2008].

## Weight Instability as a Geometric Signal

When positivity is violated, inverse probability weights diverge. Geometrically, this corresponds to disconnected regions where admissible trajectories do not exist. Weight explosion is a signal that the causal projection is being forced across regions with no valid geometric connection.


# Simulation Study

We simulated a 2D state space with survival outcomes:

- $X_1$: Systolic blood pressure (100–200 mmHg)
- $X_2$: Inflammatory marker (0–10 mg/L)
- $A$: Binary treatment (antihypertensive)
- Structural constraint: If $X_1 > 160$, then $A = 1$ (mandatory per guideline)
- Heterogeneous effect: Treatment benefit increases with $X_1$

**Estimation notes:** We enforce the structural rule $X_1 > 160 \Rightarrow A = 1$ deterministically to induce a structural positivity boundary. We estimate the hazard surfaces $\hat{\lambda}(x,a)$ using generalized additive models (GAMs) with tensor product smooths, fitting separate surfaces for $A=0$ and $A=1$. From these, we obtain $\hat{\Phi}_a(x) = -\log \hat{\lambda}(x,a)$ and compute $\Delta_A \Phi(x) = \hat{\Phi}_1(x) - \hat{\Phi}_0(x)$. We report both $\Delta_A \Phi$ and the HR field $\exp\{-\Delta_A \Phi\}$. Visualization is restricted to regions with empirical support; disconnected regions (no joint support for both $A=0$ and $A=1$) are rendered as boundaries where $\Delta_A \Phi$ is undefined. Alternative estimators (e.g., neural networks, random survival forests) could be substituted; the conceptual framework does not depend on a specific method.

**Results (N = 3,000):**

| Method | Output | Interpretation |
|--------|--------|----------------|
| Cox HR | 0.55 | Treatment reduces hazard by 45% |
| IPTW ATE | 0.20 | Treatment increases 1-year survival by 20 percentage points |
| HR field $\exp(-\Delta_A \Phi)$ | 0.19–0.66 | Effect varies ~3.5-fold across state space |

Traditional methods produce single numbers. The Field approach reveals that treatment benefit varies ~3.5-fold across the state space, with the largest effects in high-BP patients—precisely those for whom the guideline mandates treatment.

![Treatment effect field showing heterogeneity and structural boundary at $X_1 > 160$. Gray region indicates positivity violation where comparison is structurally impossible.](fig_field_approach.png){#fig-field width="90%"}

**Key Messages:**

1. Traditional methods are not wrong—Cox and IPTW correctly estimate average effects
2. But averages hide structure—HR = 0.55 does not reveal ~3.5-fold variation
3. Positivity violation is structural—the gray zone reflects clinical logic where $\Phi_a(x)$ is undefined
4. Field-based output is a map, not a number

# Estimation Details (Practical)

- Model class: Flexible regressors for $\log\lambda(x,a,t)$ (e.g., GAMs with additive smooths and selected interactions: `s(x_j)` and `ti(x_j,x_k)`) fitted separately by $a$ or jointly with interaction terms.
- Smoothing: Choose basis size $k$ via REML or cross-validation; prefer lower-order interactions to mitigate the curse of dimensionality. For $p\gg 2$, use (i) additive structures with sparse interactions, (ii) dimension reduction (PCA/PLS on $X$), or (iii) prior feature screening to stabilize surface estimation.
- Variance: Obtain uncertainty via nonparametric bootstrap over individuals and refit surfaces; propagate to $\Delta_A\Phi(x)$ pointwise. For GAMs, extract covariance of smooths for delta-method approximations where feasible.
- Sample size: For dense 2D surfaces ($k\approx 15$ per axis), $N\geq \mathcal{O}(10^3)$ is typically needed for stable maps; higher dimensions require additive structures and stronger regularization.
- Reporting: Restrict visualization to regions with empirical support for both $a=0,1$; mask others and report as structural boundaries.
- Boundary detection: (i) define common support as $\{x: n(a=0|x)\geq m \text{ and } n(a=1|x)\geq m\}$ or via propensity score trimming $\hat{e}(x)\in[\varepsilon, 1-\varepsilon]$; (ii) mask non-supported regions (NA) and render as gray in visualizations; (iii) report sensitivity to the threshold $\varepsilon$ or minimum count $m$.

# Dynamics and the Transition Kernel

For time-varying treatment, write $P(X_{t+1}\mid X_t, A_t=a)=K_a(X_t,X_{t+1})$. Policy constraints appear as zeros of $K_a$. Regime contrasts over horizons can be summarized by integrating hazards along admissible trajectories (discrete-time g-formula) and visualized as time-indexed fields $\Phi_{a,t}(x)$.


# Relation to Existing Heterogeneity Research

## Differentiation from CATE

| Aspect | CATE / HTE Research | Epidynamix |
|--------|---------------------|------------|
| Core question | "What is the effect for subgroup $X=x$?" | "What is the structure of the risk field?" |
| Output | $\tau(x) = \mathbb{E}[Y(1) - Y(0) \mid X=x]$ | $\Delta_A\Phi(x)$, structural boundaries |
| Positivity violation | Estimation problem (trim, extrapolate) | **Information** (map the boundary) |
| Counterfactuals | Explicit potential outcomes | Same logic, different framing |
| Goal | Better effect estimation | Richer output format |

**Mathematical equivalence:** For survival outcomes, $\Delta_A\Phi(x) = \log\{\lambda(x,0)/\lambda(x,1)\}$ is mathematically equivalent to CATE on the log-hazard scale. We do not claim a new estimand.

**Key distinction:** The difference is in output format and interpretation of failures. CATE treats positivity violations as missing data to be handled (trimming, extrapolation); Epidynamix reports these boundaries explicitly as structural features of the clinical landscape. Both require exchangeability; the field framing simply elevates heterogeneity and boundaries from nuisance to primary output.

## Connection to Structural Nested Models

The local effect $\Delta_A \Phi(x)$ shares conceptual ground with the "blip function" in Structural Nested Mean Models [@robins1994], which also models treatment effects as functions of patient state. The key difference is interpretive: SNMM aims to estimate causal effects under sequential exchangeability, while Epidynamix treats the effect surface as a geometric object to be mapped.

## Novelty Claim

We do not claim new mathematics or a new estimand. $\Delta_A\Phi(x)$ is CATE on a transformed scale. Potential landscapes exist in physics and systems biology (e.g., Waddington's epigenetic landscape [@waddington1957]). Concurrent work by Leizerman [@leizerman2025] develops a "Unified Causal Field Theory" using differential geometry—a more general formalization.

Our contribution is practical and interpretive:

1. **Output format:** Report the effect surface, not just its average
2. **Boundary elevation:** Treat positivity violations as structural findings, not nuisance
3. **Clinical RWD focus:** Where guidelines create structural zeros, mapping boundaries may be more informative than forcing estimation

This is a **reframing of output**, not a methodological invention.


# Discussion

The Epidynamix framework offers a complementary perspective to standard causal inference [@rubin1974; @hernan2020]. It does not replace existing methods; rather, it clarifies their domain of validity.

**When ATE works:** If the risk field is smooth, low-curvature, and well-connected (positivity holds everywhere), the ATE is an accurate summary.

**When ATE fails:** If effects are highly heterogeneous, or if structural constraints create disconnected regions, the ATE may be misleading or undefined. The field approach provides richer output—a map rather than a number.

**Practical implications:**

1. Report effect heterogeneity alongside average effects
2. Visualize the treatment effect landscape when possible
3. Treat positivity violations as findings, not errors
4. Consider whether the causal question is answerable before estimating
5. Report HR field maps ($\exp\{-\Delta_A \Phi\}$) alongside scalar summaries

**Limitations:** The framework is conceptual; practical estimation of $\Phi_a(x)$ requires flexible models (e.g., GAMs, neural networks) and careful validation. Extension to time-varying treatments and high-dimensional states remains an open challenge.


# Conclusion

Standard causal inference methods are not wrong—they provide valid average effect estimates under their assumptions. However, when effects are highly heterogeneous or structural constraints create positivity violations, a scalar summary may be insufficient.

The Epidynamix framework offers a complementary output format: mapping the treatment effect surface $\Delta_A\Phi(x)$ and explicitly reporting regions where causal comparisons are structurally impossible. This is not a new estimand but a richer way to present heterogeneity and structural boundaries.

When averages hide important structure, consider reporting the map alongside the number.


# References

::: {#refs}
:::
